{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51da76b8",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "Text Classification using RNN and Hugging Face Dataset\n",
    "\n",
    "This notebook implements a Recurrent Neural Network (RNN) for text classification using PyTorch and a dataset from Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea192b19",
   "metadata": {},
   "source": [
    "## 1. Installing and Importing Libraries\n",
    "We start by installing and importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa0c841",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b376640",
   "metadata": {},
   "source": [
    "## 2. Loading the Dataset\n",
    "We use the `imdb` dataset from Hugging Face, which consists of movie reviews labeled as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f7aca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "print(\"-\"*100)\n",
    "print(\"Dataset Strcture\")\n",
    "print(\"-\"*100)\n",
    "print(dataset)\n",
    "print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014aae7e",
   "metadata": {},
   "source": [
    "### Dataset Structure\n",
    "\n",
    "The dataset used in this notebook is the `imdb` dataset from Hugging Face. It consists of movie reviews labeled as positive or negative. The dataset is divided into three subsets:\n",
    "\n",
    "- **Train**: Contains 25,000 labeled movie reviews.\n",
    "- **Test**: Contains 25,000 labeled movie reviews.\n",
    "- **Unsupervised**: Contains 50,000 unlabeled movie reviews.\n",
    "\n",
    "Each subset has the following features:\n",
    "- `text`: The movie review text.\n",
    "- `label`: The sentiment label (0 for negative, 1 for positive)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8420d69c",
   "metadata": {},
   "source": [
    "## 3. Preprocessing the Text Data\n",
    "We use a tokenizer to convert text into sequences and prepare input tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af77a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Use a pre-trained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "print(\"Tokenizer loaded.\")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_data(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "# Apply tokenization\n",
    "dataset = dataset.map(tokenize_data, batched=True, remove_columns=[\"text\"])\n",
    "print(\"Dataset tokenized.\")\n",
    "\n",
    "# Convert dataset into PyTorch format with only necessary columns\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "print(\"Dataset format set to torch.\")\n",
    "\n",
    "# Check an example\n",
    "print(\"Example sample:\", dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563523c5",
   "metadata": {},
   "source": [
    "## 4. Creating a PyTorch Dataset and DataLoader\n",
    "We define a custom PyTorch dataset class and create data loaders.\n",
    "\n",
    "Creating a custom PyTorch dataset class and data loaders is essential for efficiently handling and processing the data during training and evaluation. The custom dataset class allows us to define how the data is accessed and transformed, ensuring that each sample is correctly formatted for the model. Data loaders facilitate batching, shuffling, and parallel data loading, which are crucial for optimizing the training process and improving model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9340a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        print(f\"Initialized IMDBDataset with {len(data)} samples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        length = len(self.data)\n",
    "        return length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        input_ids = torch.tensor(item.get(\"input_ids\", [0] * 256), dtype=torch.long)\n",
    "        attention_mask = torch.tensor(item.get(\"attention_mask\", [0] * 256), dtype=torch.long)\n",
    "        label = torch.tensor(item.get(\"label\", 0), dtype=torch.float)\n",
    "\n",
    "        result = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"label\": label\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Creating data loaders\n",
    "batch_size = 16\n",
    "\n",
    "print(\"Creating train_loader...\")\n",
    "train_loader = DataLoader(IMDBDataset(dataset[\"train\"]), batch_size=batch_size, shuffle=True)\n",
    "print(\"train_loader created.\")\n",
    "\n",
    "print(\"Creating test_loader...\")\n",
    "test_loader = DataLoader(IMDBDataset(dataset[\"test\"]), batch_size=batch_size, shuffle=False)\n",
    "print(\"test_loader created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe6c99d",
   "metadata": {},
   "source": [
    "## 5. Defining the RNN Model\n",
    "We define a simple RNN model with an embedding layer, an RNN layer, and a fully connected output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1525aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Recurrent Neural Network (RNN) model for binary text classification.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): The size of the vocabulary.\n",
    "        embed_dim (int): The dimensionality of the embedding layer.\n",
    "        hidden_dim (int): The number of features in the hidden state of the RNN.\n",
    "        output_dim (int): The number of output features (1 for binary classification).\n",
    "\n",
    "    Attributes:\n",
    "        embedding (nn.Embedding): The embedding layer that converts input tokens to dense vectors.\n",
    "        rnn (nn.RNN): The RNN layer that processes the embedded input sequences.\n",
    "        fc (nn.Linear): The fully connected layer that maps the RNN output to the desired output dimension.\n",
    "        sigmoid (nn.Sigmoid): The sigmoid activation function applied to the output of the fully connected layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor containing token IDs.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor containing the predicted probabilities.\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)  # Convert input tokens to dense vectors\n",
    "        x, _ = self.rnn(x)     # Process the embedded input sequences with the RNN\n",
    "        x = self.fc(x[:, -1, :])  # Use the output of the last time step\n",
    "        return self.sigmoid(x)  # Apply sigmoid activation to get probabilities\n",
    "\n",
    "# Model Initialization\n",
    "vocab_size = tokenizer.vocab_size  # Size of the vocabulary from the tokenizer\n",
    "embed_dim = 128  # Dimensionality of the embedding layer\n",
    "hidden_dim = 64  # Number of features in the hidden state of the RNN\n",
    "output_dim = 1  # Number of output features (1 for binary classification)\n",
    "\n",
    "model = RNNModel(vocab_size, embed_dim, hidden_dim, output_dim)\n",
    "print(\"-\"*100)\n",
    "print(\"Model Architecture\")\n",
    "print(\"-\"*100)\n",
    "print(model)\n",
    "print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957dbe4c",
   "metadata": {},
   "source": [
    "## 6. Training the Model\n",
    "We train the model using Binary Cross Entropy loss and the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b48ea03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/1563 [00:00<?, ?it/s]C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_16772\\923347437.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(item.get(\"input_ids\", [0] * 256), dtype=torch.long)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_16772\\923347437.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask = torch.tensor(item.get(\"attention_mask\", [0] * 256), dtype=torch.long)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_16772\\923347437.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(item.get(\"label\", 0), dtype=torch.float)\n",
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Loss: 0.6964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 - Loss: 0.6895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 - Loss: 0.6729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 - Loss: 0.6443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 - Loss: 0.6275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "epochs = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"label\"].float().to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=total_loss/len(train_loader))\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc0c182",
   "metadata": {},
   "source": [
    "## 7. Evaluating the Model\n",
    "We evaluate the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "261d5d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_16772\\923347437.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(item.get(\"input_ids\", [0] * 256), dtype=torch.long)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_16772\\923347437.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask = torch.tensor(item.get(\"attention_mask\", [0] * 256), dtype=torch.long)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_16772\\923347437.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(item.get(\"label\", 0), dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 53.06%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Function\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            outputs = model(input_ids).squeeze()\n",
    "            predictions = (outputs > 0.5).float()\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecb8121",
   "metadata": {},
   "source": [
    "## 8. Making Predictions\n",
    "We define a function to predict the sentiment of a given text input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96d13153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "# Prediction Function\n",
    "def predict_sentiment(text):\n",
    "    model.eval()\n",
    "    encoded_input = tokenizer(text, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "    input_ids = encoded_input[\"input_ids\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        prediction = model(input_ids).item()\n",
    "    \n",
    "    return \"Positive\" if prediction > 0.5 else \"Negative\"\n",
    "\n",
    "# Example Prediction\n",
    "print(predict_sentiment(\"I love this movie!\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
